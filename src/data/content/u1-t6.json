{
    "id": "u1-t6",
    "title": "Probabilistic Models",
    "unitId": "unit-1",
    "slides": [
        {
            "slideNumber": 1,
            "type": "title",
            "title": "Probabilistic Information Retrieval",
            "subtitle": "Binary Independence Model & BM25",
            "content": {
                "text": "Using Probability Theory to estimate the likelihood that a user will find a document relevant.",
                "hook": "We don't just guess relationships; we calculate the odds."
            }
        },
        {
            "slideNumber": 2,
            "type": "standard",
            "title": "Probability Ranking Principle (PRP)",
            "content": {
                "definition": "PRP: 'If a reference retrieval system's response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance... then the system's effectiveness will be the best that can be obtained.'",
                "text": "Basically: Rank by P(Rel|Doc)."
            }
        },
        {
            "slideNumber": 3,
            "type": "standard",
            "title": "Binary Independence Model (BIM)",
            "content": {
                "text": "The classic probabilistic model. 'Binary' because terms are either present (1) or absent (0). 'Independence' because we assume terms occur independently (like Naive Bayes).",
                "problem": "Requires initial guesses for P(Relevant), which is hard to get without user feedback."
            }
        },
        {
            "slideNumber": 4,
            "type": "list",
            "title": "Key Components of BIM",
            "items": [
                "P(ti | R): Probability term i occurs in a relevant doc.",
                "P(ti | NR): Probability term i occurs in a non-relevant doc.",
                "Log-Odds Ratio: The weight of a term is derived from these probabilities.",
                "Retrieval Status Value (RSV): The score of a document."
            ]
        },
        {
            "slideNumber": 5,
            "type": "standard",
            "title": "Okapi BM25",
            "content": {
                "definition": "Best Match 25. A non-binary probabilistic model that incorporates Term Frequency and Document Length Normalization.",
                "text": "The gold standard in IR for decades. It fixes BIM's flaw (ignoring frequency) and VSM's flaw (ignoring document length bias)."
            },
            "formula": {
                "equation": "\\text{BM25}(q, d) = \\sum_{t \\in q} \\text{IDF}(t) \\cdot \\frac{tf_{t,d} \\cdot (k_1 + 1)}{tf_{t,d} + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{\\text{avgdl}})}",
                "description": "BM25 (Best Matching 25) is the state-of-the-art probabilistic ranking function combining TF, IDF, and document length normalization.",
                "terms": [
                    {
                        "symbol": "\\text{BM25}(q, d)",
                        "meaning": "Relevance score for document d given query q - higher score means more relevant"
                    },
                    {
                        "symbol": "t \\in q",
                        "meaning": "Summation over all terms t that appear in query q"
                    },
                    {
                        "symbol": "\\text{IDF}(t)",
                        "meaning": "Inverse Document Frequency of term t - typically log((N - df_t + 0.5) / (df_t + 0.5))"
                    },
                    {
                        "symbol": "tf_{t,d}",
                        "meaning": "Raw term frequency - number of times term t appears in document d"
                    },
                    {
                        "symbol": "k_1",
                        "meaning": "Term frequency saturation parameter (typical value: 1.2 to 2.0) - controls how quickly TF impact saturates"
                    },
                    {
                        "symbol": "b",
                        "meaning": "Document length normalization parameter (typical value: 0.75) - 0 means no normalization, 1 means full normalization"
                    },
                    {
                        "symbol": "|d|",
                        "meaning": "Length of document d in words"
                    },
                    {
                        "symbol": "\\text{avgdl}",
                        "meaning": "Average document length across the entire collection"
                    },
                    {
                        "symbol": "\\frac{|d|}{\\text{avgdl}}",
                        "meaning": "Document length ratio - penalizes long documents, boosts short ones"
                    }
                ]
            }
        },
        {
            "slideNumber": 6,
            "type": "illustration",
            "title": "BM25 Probabilistic Ranking",
            "content": {
                "description": "Logic of the state-of-the-art probabilistic ranking.",
                "steps": [
                    "Query Term Match",
                    "Inverse Document Frequency (IDF)",
                    "Term Frequency (TF) with Saturation",
                    "Document Length Normalization",
                    "Aggregate Ranked Potential"
                ]
            }
        },
        {
            "slideNumber": 7,
            "type": "diagram",
            "title": "Bayesian Network Logic",
            "content": {
                "description": "Modeling retrieval as belief propagation.",
                "steps": [
                    "Document Layer (Sources)",
                    "Concept Layer (Index Terms)",
                    "Query Layer (User Goal)",
                    "Inference Pass (Relevance Belief)"
                ]
            }
        },
        {
            "slideNumber": 8,
            "type": "activity",
            "title": "Activity: Odds Calculation",
            "content": {
                "activity": "Calculate Weights",
                "description": "Given: Collection size N=100. Term 'cloud' appears in 30 docs. Assume relevant set R=10 docs, and 'cloud' is in 8 of them. Calculate the probabilistic weight for 'cloud'."
            }
        },
        {
            "slideNumber": 9,
            "type": "project",
            "title": "Mini-Project: BM25 Scorer",
            "content": {
                "idea": "Implement the Formula",
                "input": "Query 'q', Doc 'd', AvgDocLength, k1=1.5, b=0.75.",
                "process": "Write a Python function `bm25_score(q, d)`.",
                "output": "Compare scores against standard TF-IDF."
            }
        },
        {
            "slideNumber": 10,
            "type": "quiz",
            "title": "Quiz",
            "questions": [
                "What assumption does the Binary Independence Model make about term relationships?",
                "What prevents a long document from unfairly getting a high score in BM25?",
                "What is 'Term Frequency Saturation'?",
                "Explain the role of Relevance Feedback in probabilistic models."
            ],
            "answers": [
                "It assumes terms occur independently of each other within relevant and non-relevant sets (similar to Naive Bayes), ignoring term dependencies.",
                "Document Length Normalization (parameter 'b'). It penalizes term frequency counts from very long documents to allow fair comparison with shorter ones.",
                "A property where increasing the number of times a term appears in a document yields diminishing returns in score, unlike raw TF which grows linearly.",
                "It provides the initial (or updated) set of relevant documents needed to estimate the probabilities P(term|Relevant) and P(term|Non-Relevant) accurately."
            ]
        },
        {
            "slideNumber": 11,
            "type": "summary",
            "title": "Topic Wrap-up",
            "content": {
                "linkage": "We have covered flat text models. But real data has structure (Title vs Body, XML tags).",
                "nextTopic": "Structured Text Retrieval",
                "preparation": "Familiarize yourself with DOM trees and XML paths."
            }
        }
    ]
}