{
    "id": "u2-t3",
    "title": "Text Operations (Preprocessing, Clustering, Compression)",
    "unitId": "unit-2",
    "slides": [
        {
            "slideNumber": 1,
            "type": "title",
            "title": "Text Operations",
            "subtitle": "Transforming Raw Text for Storage and Analysis",
            "content": {
                "text": "Text operations prepare documents for indexing. This involves cleaning (preprocessing), grouping (clustering), and shrinking (compression).",
                "hook": "Data is messy; information is organized; compressed information is efficient."
            }
        },
        {
            "slideNumber": 2,
            "type": "diagram",
            "title": "Text Processing Pipeline",
            "content": {
                "description": "The logic of preparing raw documents for indexing.",
                "steps": [
                    "Tokenization (Lexical Analysis)",
                    "Stopword Filtering",
                    "Stemming / Lemmatization",
                    "Thesaurus Expansion",
                    "Term Normalization"
                ]
            }
        },
        {
            "slideNumber": 3,
            "type": "standard",
            "title": "Document Clustering",
            "content": {
                "definition": "Unsupervised grouping of documents based on content similarity.",
                "text": "Clusters help in: 1. Speeding up retrieval (Cluster Pruning), 2. Organizing results for the user, 3. Detecting duplicate content.",
                "hook": "Birds of a feather (or words of a topic) flock together."
            }
        },
        {
            "slideNumber": 4,
            "type": "list",
            "title": "Text Compression",
            "items": [
                "Objective: Reduce storage and increase disk transfer speed.",
                "Huffman Coding: Variable-length prefix codes.",
                "Arithmetic Coding: Better compression than Huffman but slower.",
                "Ziv-Lempel (LZ) algorithms: Dictionary-based compression (ZIP/GZIP)."
            ],
            "formula": {
                "equation": "H = - \\sum_{i} P(s_i) \\log_2 P(s_i)",
                "description": "Entropy (H) defines the theoretical limit of text compression. Huffman coding calculates bit-lengths for symbols based on their probability (P) to approach this limit.",
                "terms": [
                    {
                        "symbol": "H",
                        "meaning": "Entropy - average bits required per symbol"
                    },
                    {
                        "symbol": "P(s_i)",
                        "meaning": "Probability of occurrence for symbol 'i' in the text"
                    },
                    {
                        "symbol": "\\log_2",
                        "meaning": "Logarithm base 2 (yields units of bits)"
                    }
                ],
                "calculation": {
                    "exampleTitle": "Huffman Bit-Savings",
                    "description": "Comparing fixed-length vs variable-length for 3 symbols.",
                    "steps": [
                        {
                            "label": "Frequency Count",
                            "formula": "A=60\\%, B=30\\%, C=10\\%",
                            "note": "'A' is the most common character."
                        },
                        {
                            "label": "Assign Codes",
                            "formula": "A=0, B=10, C=11",
                            "note": "Common symbols get shorter codes."
                        },
                        {
                            "label": "Average Length",
                            "formula": "L = (0.6 \\times 1) + (0.3 \\times 2) + (0.1 \\times 2) = 1.4 \\text{ bits}",
                            "note": "A fixed 2-bit code would use 2.0 bits."
                        },
                        {
                            "label": "Space Saved",
                            "formula": "\\text{Reduction} = 30\\%",
                            "note": "Saving 0.6 bits per character (30% less space)."
                        }
                    ],
                    "input": "Probabilities: [0.6, 0.3, 0.1]",
                    "output": "Avg Length: 1.4 bits"
                }
            }
        },
        {
            "slideNumber": 5,
            "type": "standard",
            "title": "Comparing Compression Techniques",
            "content": {
                "text": "How do we choose a compression method?",
                "outcomes": [
                    "Compression Ratio: How much space is saved.",
                    "Encoding Speed: How fast can we compress.",
                    "Decoding Speed: How fast can we read (critical for search).",
                    "Random Access: Can we jump to the middle without decompressing everything?"
                ]
            }
        },
        {
            "slideNumber": 6,
            "type": "activity",
            "title": "Activity: Huffman Tree",
            "content": {
                "activity": "Code the Word",
                "description": "Calculate the frequency of letters in the word 'MISSISSIPPI'. Draw a simple Huffman tree. Which letter gets the shortest code?"
            }
        },
        {
            "slideNumber": 7,
            "type": "project",
            "title": "Mini-Project: Tokenizer",
            "content": {
                "idea": "Clean the web",
                "input": "A raw HTML file.",
                "process": "1. Strip tags, 2. Lowercase, 3. Remove punctuation, 4. Stem.",
                "output": "A list of valid indexable tokens."
            }
        },
        {
            "slideNumber": 8,
            "type": "quiz",
            "title": "Topic Quiz",
            "questions": [
                "What is the difference between stemming and lemmatization?",
                "Name one benefit of document clustering in search engines.",
                "Why is decoding speed more important than encoding speed in IR?",
                "How does Zipf's law relate to stopword removal?"
            ]
        },
        {
            "slideNumber": 9,
            "type": "python_demo",
            "title": "Python Demo: Cosine Similarity",
            "content": {
                "code": "import math\n\ndef cosine_sim(v1, v2):\n    dot = sum(a*b for a, b in zip(v1, v2))\n    norm1 = math.sqrt(sum(a*a for a in v1))\n    norm2 = math.sqrt(sum(b*b for b in v2))\n    return dot / (norm1 * norm2) if norm1*norm2 > 0 else 0\n\nq = [1, 0, 1]\nd = [1, 1, 0]\n\nsim = cosine_sim(q, d)\nprint(f\"Similarity: {round(sim, 3)}\")",
                "input": "Query: [1, 0, 1]\nDoc: [1, 1, 0]",
                "output": "Similarity: 0.5",
                "interpretation": "Cosine similarity measures the angle between vectors.\nA score of 0.5 means the query and document vectors are 60 degrees apart.\nThis is the standard ranking metric for the Vector Space Model."
            }
        },
        {
            "slideNumber": 10,
            "type": "summary",
            "title": "Wrap-up",
            "content": {
                "text": "Text is now ready. Next, we build the actual structure that allows us to find it: the Inverted Index.",
                "nextTopic": "Indexing and Searching",
                "preparation": "Learn about the structure of an Inverted File."
            }
        }
    ]
}