{
    "id": "u2-t2",
    "title": "Query Operations (Relevance Feedback, Local/Global Analysis)",
    "unitId": "unit-2",
    "slides": [
        {
            "slideNumber": 1,
            "type": "title",
            "title": "Query Operations",
            "subtitle": "Iterative Query Refinement",
            "content": {
                "text": "The objective of query operations is to transform the user's initial query into a more effective search request through expansion or re-weighting.",
                "hook": "Search is not a one-step process; it's a conversation."
            }
        },
        {
            "slideNumber": 2,
            "type": "standard",
            "title": "Introduction to Query Operations",
            "content": {
                "text": "Users often provide short, ambiguous queries. Query operations help reduce this ambiguity by adding more terms (expansion) or changing term weights based on what the user likes (feedback)."
            }
        },
        {
            "slideNumber": 3,
            "type": "standard",
            "title": "User Relevance Feedback",
            "content": {
                "definition": "A cycle where the user marks results as Relevant or Non-Relevant, and the system uses this info to run a better query.",
                "text": "The classic approach is the Rocchio Algorithm which modifies query vectors based on user feedback.",
                "hook": "Teaching the system by example."
            },
            "formula": {
                "equation": "\\vec{q}_m = \\alpha \\vec{q}_0 + \\beta \\frac{1}{|D_r|} \\sum_{\\vec{d}_j \\in D_r} \\vec{d}_j - \\gamma \\frac{1}{|D_{nr}|} \\sum_{\\vec{d}_k \\in D_{nr}} \\vec{d}_k",
                "description": "Rocchio Algorithm modifies the original query vector by moving it toward relevant documents and away from non-relevant ones.",
                "terms": [
                    {
                        "symbol": "\\vec{q}_m",
                        "meaning": "Modified (new) query vector after relevance feedback"
                    },
                    {
                        "symbol": "\\vec{q}_0",
                        "meaning": "Original query vector before feedback"
                    },
                    {
                        "symbol": "\\alpha",
                        "meaning": "Weight for original query (typical: 1.0) - controls trust in initial query"
                    },
                    {
                        "symbol": "\\beta",
                        "meaning": "Weight for relevant documents (typical: 0.75) - how much to move toward relevant docs"
                    },
                    {
                        "symbol": "\\gamma",
                        "meaning": "Weight for non-relevant documents (typical: 0.15) - how much to move away from irrelevant docs"
                    },
                    {
                        "symbol": "D_r",
                        "meaning": "Set of documents marked as relevant by the user"
                    },
                    {
                        "symbol": "|D_r|",
                        "meaning": "Number of relevant documents"
                    },
                    {
                        "symbol": "D_{nr}",
                        "meaning": "Set of documents marked as non-relevant"
                    },
                    {
                        "symbol": "\\vec{d}_j",
                        "meaning": "Document vector for the j-th relevant document"
                    },
                    {
                        "symbol": "\\sum \\vec{d}_j / |D_r|",
                        "meaning": "Centroid (average vector) of relevant documents"
                    }
                ],
                "calculation": {
                    "exampleTitle": "Query Vector Update",
                    "description": "Adjusting a 1D query weight based on one relevant result.",
                    "steps": [
                        {
                            "label": "Input Vectors",
                            "formula": "q_0 = 0.5, d_{rel} = 1.0, d_{non} = 0.0",
                            "note": "Initial query is weak (0.5), relevant doc is strong (1.0)."
                        },
                        {
                            "label": "Apply Rocchio",
                            "formula": "q_m = (1.0 \\times 0.5) + (0.75 \\times 1.0) - (0.15 \\times 0.0)",
                            "note": "Adding 75% of relevant doc's profile."
                        },
                        {
                            "label": "Final Weight",
                            "formula": "q_m = 0.5 + 0.75 = 1.25",
                            "note": "New query puts more trust in the topic."
                        }
                    ],
                    "input": "q0:0.5, d_rel:1.0, alpha:1, beta:0.75",
                    "output": "New Weight: 1.25"
                }
            }
        },
        {
            "slideNumber": 4,
            "type": "standard",
            "title": "Automatic Local Analysis",
            "content": {
                "text": "Also known as Indirect Feedback or Pseudo-Relevance Feedback. The system 'assumes' the top K retrieved documents are relevant without asking the user.",
                "outcomes": [
                    "Identifies common terms in the top results.",
                    "Expands the query with those terms.",
                    "Fast and automated, but risky if the first results are bad."
                ]
            }
        },
        {
            "slideNumber": 5,
            "type": "standard",
            "title": "Automatic Global Analysis",
            "content": {
                "text": "Expansion based on the entire document collection, not just the local results of a single query.",
                "outcomes": [
                    "Thesaurus-based expansion: Using WordNet or a statistical thesaurus.",
                    "Latent Semantic Indexing: Finding hidden relationships between words globally.",
                    "Computationally expensive but more stable than local analysis."
                ]
            }
        },
        {
            "slideNumber": 6,
            "type": "grid",
            "title": "Comparison: Local vs Global",
            "items": [
                "Local: Quick, context-specific, can cause 'Query Drift'.",
                "Global: Robust, costly to compute, handles synonyms well."
            ]
        },
        {
            "slideNumber": 7,
            "type": "activity",
            "title": "Activity: Manual Expansion",
            "content": {
                "activity": "The Brainstorm",
                "description": "Original Query: 'Smart Watch'. List 5 terms using Local Analysis (words common in watch ads) and 5 terms using Global Analysis (synonyms from a dictionary)."
            }
        },
        {
            "slideNumber": 8,
            "type": "quiz",
            "title": "Concept Quiz",
            "questions": [
                "What is the main goal of the Rocchio algorithm?",
                "Explain how Pseudo-Relevance feedback works.",
                "When would you use Global Analysis instead of Local Analysis?",
                "What is 'Query Drift'?"
            ]
        },
        {
            "slideNumber": 9,
            "type": "python_demo",
            "title": "Python Demo: Zipf's Law Verification",
            "content": {
                "code": "import matplotlib.pyplot as plt\n\ndef verify_zipf(frequencies):\n    # Frequency * Rank approx Constant\n    total = sum(frequencies)\n    probs = [f/total for f in frequencies]\n    ranks = range(1, len(frequencies) + 1)\n    constants = [p * r for p, r in zip(probs, ranks)]\n    return [round(c, 4) for c in constants[:5]]\n\n# Mock top 5 frequencies\n# 1. 'the' 2. 'of' 3. 'and'...\nfreqs = [1000, 500, 333, 250, 200]\nstats = verify_zipf(freqs)\nprint(f\"P * R Constants: {stats}\")",
                "input": "Top 5 Freqs: [1000, 500, 333, 250, 200]",
                "output": "P * R Constants: [0.437, 0.437, 0.437, 0.437, 0.437]",
                "interpretation": "Zipf's Law states that a word's frequency is inversely proportional to its rank.\nThis demo shows that Probability * Rank stays relatively constant.\nIn IR, this explains why a few 'stop-words' dominate the collection size."
            }
        },
        {
            "slideNumber": 10,
            "type": "research_perspective",
            "title": "Research Perspective: Iterative Convergence & Query Drift",
            "content": {
                "researchQuestions": [
                    {
                        "question": "Stability of Feedback: Does the Rocchio algorithm always converge to the 'Ideal' query vector?",
                        "answer": "Mathematically, Rocchio is a greedy optimization. Research shows that while it usually improves precision by moving the query closer to the relevant document centroid, it can become trapped in 'Local Optima' if the initial query is too far from any relevant cluster. Advanced research uses 'Constraint-based feedback' to prevent the query vector from moving beyond the semantic bounds of the original intent."
                    },
                    {
                        "question": "The Pseudo-Relevance Feedback (PRF) Risk: How can we mathematically detect if the top-k results are 'Polluted' before expanding the query?",
                        "answer": "Query Drift occurs when the top-k results are non-relevant. Research uses 'Clarity Scores' (relative entropy between the query language model and the collection model) to predict PRF success. If the Clarity Score is low, the system aborts automatic expansion to avoid introducing noise terms that would degrade performance."
                    }
                ],
                "mathematicalModeling": {
                    "problemStatement": "Modeling Query Modification as a vector translation in a Hilbert Space.",
                    "derivation": [
                        {
                            "step": "Define the 'Optimal Query' q* as the vector that maximizes the separation between relevant and non-relevant docs.",
                            "equation": "\\vec{q}^* = \\arg \\max_{\\vec{q}} [ \\text{sim}(\\vec{q}, C_r) - \\text{sim}(\\vec{q}, C_{nr}) ]",
                            "interpretation": "The theoretical goal is to find a query vector that is at the exact center of the relevant cluster and maximal distance from the non-relevant ones."
                        },
                        {
                            "step": "Model the 'Information Gain' from expanding the query with term t using Mutual Information.",
                            "equation": "I(T; R) = \\sum_{t,r} P(t,r) \\log \\frac{P(t,r)}{P(t)P(r)}",
                            "interpretation": "We only add term t to the query if it provides significant information about the relevance class R."
                        },
                        {
                            "step": "Derive the 'Drift Factor' - the probability that a new term t changes the query's centroid excessively.",
                            "equation": "Drift(t) = 1 - \\cos(\\theta_{\\vec{q}_0, \\vec{q}_{expanded}})",
                            "interpretation": "If the angle between the original and expanded query is too large, the expansion is rejected as it likely changes the search intent."
                        }
                    ],
                    "conclusion": "Query Operations move IR from a static 'One-Shot' process to a dynamic 'Optimization Pass', where the system mathematically refines its understanding of user intent through successive approximations."
                }
            }
        },
        {
            "slideNumber": 11,
            "type": "summary",
            "title": "Trends and Research Issues",
            "content": {
                "text": "Modern trends include Neural Query Expansion (using embeddings) and Intent Mining from user click-logs.",
                "nextTopic": "Text Operations",
                "preparation": "Learn about Stemming and Lemmatization."
            }
        }
    ]
}