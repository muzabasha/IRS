{
    "id": "u1-t3",
    "title": "Classic IR Models",
    "unitId": "unit-1",
    "slides": [
        {
            "slideNumber": 1,
            "type": "title",
            "title": "Classic Information Retrieval Models",
            "subtitle": "Boolean, Vector, and Probabilistic",
            "content": {
                "text": "IR Models provide the mathematical framework for defining the retrieval process. The 'Classic' models are the foundation of all modern search systems.",
                "hook": "How do you mathematically prove that Document A is more relevant than Document B?"
            }
        },
        {
            "slideNumber": 2,
            "type": "diagram",
            "title": "Taxonomy of IR Models",
            "content": {
                "description": "The family tree of retrieval models.",
                "steps": [
                    "Set-Theoretic (Boolean)",
                    "Algebraic (Vector Space)",
                    "Probabilistic (BM25)"
                ]
            }
        },
        {
            "slideNumber": 3,
            "type": "standard",
            "title": "The Boolean Model",
            "content": {
                "definition": "A model based on set theory and Boolean algebra. Documents are sets of terms. Queries are boolean expressions (AND, OR, NOT).",
                "text": "The simplest model. A document either matches (1) or it doesn't (0). There is no partial match."
            }
        },
        {
            "slideNumber": 4,
            "type": "list",
            "title": "Boolean Model: Pros & Cons",
            "items": [
                "✅ Easy to implement and computationally efficient.",
                "✅ Exact semantics (User gets exactly what they asked for).",
                "❌ No ranking (Alphabetical sort is useless for large results).",
                "❌ 'Too strict': AND can yield zero results; OR can yield thousands."
            ]
        },
        {
            "slideNumber": 5,
            "type": "standard",
            "title": "The Vector Space Model (VSM)",
            "content": {
                "definition": "Visualizes documents and queries as vectors in a multi-dimensional space. Relevance is the distance/angle between them.",
                "text": "Solves the ranking problem. Uses term weights (TF-IDF) to determine importance. We compute the Cosine Similarity between the Query Vector and Document Vectors."
            },
            "formula": {
                "equation": "\\text{sim}(\\vec{q}, \\vec{d}) = \\frac{\\vec{q} \\cdot \\vec{d}}{||\\vec{q}|| \\times ||\\vec{d}||} = \\frac{\\sum_{i=1}^{n} q_i \\times d_i}{\\sqrt{\\sum_{i=1}^{n} q_i^2} \\times \\sqrt{\\sum_{i=1}^{n} d_i^2}}",
                "description": "Cosine Similarity measures the angle between query and document vectors in the high-dimensional term space.",
                "terms": [
                    {
                        "symbol": "\\text{sim}(\\vec{q}, \\vec{d})",
                        "meaning": "Similarity score between query vector q and document vector d (range: 0 to 1)"
                    },
                    {
                        "symbol": "\\vec{q} \\cdot \\vec{d}",
                        "meaning": "Dot product of query and document vectors - measures overlap in term space"
                    },
                    {
                        "symbol": "||\\vec{q}||",
                        "meaning": "Magnitude (length) of query vector - normalizes for query length"
                    },
                    {
                        "symbol": "||\\vec{d}||",
                        "meaning": "Magnitude (length) of document vector - normalizes for document length"
                    },
                    {
                        "symbol": "q_i",
                        "meaning": "Weight of the i-th term in the query (typically TF-IDF)"
                    },
                    {
                        "symbol": "d_i",
                        "meaning": "Weight of the i-th term in the document (typically TF-IDF)"
                    },
                    {
                        "symbol": "n",
                        "meaning": "Total number of unique terms in the vocabulary"
                    }
                ]
            },
            "calculation": {
                "exampleTitle": "Cosine Similarity Example",
                "description": "Calculating the similarity between a query Q and a document D1 in a 2D term space.",
                "steps": [
                    {
                        "label": "Define Vectors",
                        "formula": "\\vec{q} = [1, 1], \\vec{d} = [2, 0]",
                        "note": "Terms: 'search', 'engine'"
                    },
                    {
                        "label": "Calculate Dot Product",
                        "formula": "\\vec{q} \\cdot \\vec{d} = (1 \\times 2) + (1 \\times 0) = 2"
                    },
                    {
                        "label": "Calculate Magnitudes",
                        "formula": "||\\vec{q}|| = \\sqrt{1^2 + 1^2} = 1.414, ||\\vec{d}|| = \\sqrt{2^2 + 0^2} = 2"
                    },
                    {
                        "label": "Final Score",
                        "formula": "\\text{sim} = \\frac{2}{1.414 \\times 2} = 0.707"
                    }
                ],
                "input": "Q:[1,1], D:[2,0]",
                "output": "Similarity: 0.707"
            },
            "subTopics": [
                {
                    "title": "Deep Dive: Document Vectorization",
                    "content": "A document is represented as a point in a n-dimensional space where each dimension corresponds to a unique term in the vocabulary.",
                    "subSubTopics": [
                        {
                            "title": "Normalized Term Weights",
                            "content": "Why normalize? Without normalization, long documents would naturally have higher similarity to any query just because they have more words.",
                            "keyPoints": [
                                "Cosine normalization: Divides by the length of the vector.",
                                "Ensures all documents have equal competing chance regardless of length."
                            ]
                        }
                    ]
                },
                {
                    "title": "Deep Dive: Thresholding & Ranking",
                    "content": "Instead of a binary yes/no, VSM allows us to set a threshold for similarity.",
                    "subSubTopics": [
                        {
                            "title": "Optimal Thresholding",
                            "content": "Finding the 'sweet spot' where we keep only the most relevant results.",
                            "examples": [
                                "Setting threshold = 0.5 to filter low-confidence matches.",
                                "Using top-K results for dynamic thresholding."
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "slideNumber": 6,
            "type": "illustration",
            "title": "Vector Space Representation",
            "content": {
                "description": "The logic of geometric retrieval.",
                "steps": [
                    "Term Dimensions (X, Y, Z axes)",
                    "Document Vectorization",
                    "Query Vectorization",
                    "Cosine Angle Calculation"
                ]
            }
        },
        {
            "slideNumber": 7,
            "type": "standard",
            "title": "TF-IDF Weighting",
            "content": {
                "text": "The most famous formula in IR.",
                "outcomes": [
                    "TF (Term Frequency): How often term t appears in doc d. (Higher is better)",
                    "IDF (Inverse Document Frequency): log(N/df). Penalizes common words like 'the'. (Rarity is valuable)"
                ]
            },
            "formula": {
                "equation": "w_{t,d} = tf_{t,d} \\times \\log\\frac{N}{df_t}",
                "description": "TF-IDF (Term Frequency-Inverse Document Frequency) is the fundamental weighting scheme in Information Retrieval.",
                "terms": [
                    {
                        "symbol": "w_{t,d}",
                        "meaning": "Weight of term t in document d - the final importance score"
                    },
                    {
                        "symbol": "tf_{t,d}",
                        "meaning": "Term Frequency - number of times term t appears in document d"
                    },
                    {
                        "symbol": "N",
                        "meaning": "Total number of documents in the collection"
                    },
                    {
                        "symbol": "df_t",
                        "meaning": "Document Frequency - number of documents containing term t"
                    },
                    {
                        "symbol": "\\log\\frac{N}{df_t}",
                        "meaning": "IDF (Inverse Document Frequency) - penalizes common terms, boosts rare discriminative terms"
                    }
                ],
                "calculation": {
                    "exampleTitle": "TF-IDF Weighting Example",
                    "description": "Weighting the term 'Retrieval' in a document within a corpus of 10,000 documents.",
                    "steps": [
                        {
                            "label": "Term Frequency (TF)",
                            "formula": "tf_{t,d} = 5",
                            "note": "The word appears 5 times in this document."
                        },
                        {
                            "label": "Document Frequency (DF)",
                            "formula": "df_t = 100",
                            "note": "100 documents in the entire collection contain this word."
                        },
                        {
                            "label": "Inverse Document Frequency (IDF)",
                            "formula": "idf = \\log_{10}(\\frac{10000}{100}) = 2",
                            "note": "Using base-10 log for simplicity."
                        },
                        {
                            "label": "Final Weight",
                            "formula": "w = 5 \\times 2 = 10",
                            "note": "Term Frequency multiplied by IDF."
                        }
                    ],
                    "input": "TF:5, DF:100, N:10000",
                    "output": "Weight (w): 10.0"
                }
            }
        },
        {
            "slideNumber": 8,
            "type": "standard",
            "title": "Probabilistic Model (Binary Independence)",
            "content": {
                "text": "Based on the Probability Ranking Principle: 'If a system ranks documents by decreasing probability of relevance to the user, effectiveness is maximized.'",
                "hook": "It asks: What are the odds that this document is relevant?"
            }
        },
        {
            "slideNumber": 9,
            "type": "grid",
            "title": "Model Comparison",
            "items": [
                "Boolean: Binary decision. Good for expert systems (Law, Med).",
                "Vector: Ranked results. Best for general web search.",
                "Probabilistic: Ranked results. Strong theoretical basis, but hard to estimate initial probabilities."
            ]
        },
        {
            "slideNumber": 10,
            "type": "activity",
            "title": "Activity: Manual Indexing",
            "content": {
                "activity": "Rank the Docs",
                "description": "Query: 'Apple Pie'. Doc A: 'Apple chart pie'. Doc B: 'Apple apple apple'. Doc C: 'Pie chart'. Which is most relevant in Boolean? Which in VSM?"
            }
        },
        {
            "slideNumber": 11,
            "type": "project",
            "title": "Project: VSM Calculator",
            "content": {
                "idea": "Cosine Similarity Engine",
                "input": "Query 'Q' and two sentences 'D1', 'D2'.",
                "process": "1. Count word frequencies (TF). 2. Compute Cosine Similarity.",
                "output": "Score for D1 vs D2. Declare the winner."
            }
        },
        {
            "slideNumber": 12,
            "type": "quiz",
            "title": "Mastery Check",
            "questions": [
                "Why is the Boolean model considered 'data retrieval' rather than 'information retrieval'?",
                "What does IDF stand for, and why is it needed?",
                "If the angle between two document vectors is 0 degrees, what is their similarity?",
                "State the Probability Ranking Principle."
            ],
            "answers": [
                "Because it performs exact matching (binary outcome) without ranking, similar to database queries, rather than estimating relevance.",
                "Inverse Document Frequency. It is needed to penalize common words (like 'the') and boost rare, discriminative terms to improve retrieval quality.",
                "The similarity is 1 (Perfect Similarity), as Cosine(0) = 1.",
                "\"If a system ranks documents by decreasing probability of relevance to the user, effectiveness is maximized.\""
            ]
        },
        {
            "slideNumber": 13,
            "type": "python_demo",
            "title": "Python Demo: Model Comparison",
            "content": {
                "code": "def boolean_match(query_terms, doc_terms):\n    return set(query_terms).issubset(set(doc_terms))\n\nquery = ['information', 'retrieval']\ndoc1 = ['this', 'is', 'an', 'information', 'retrieval', 'system']\ndoc2 = ['only', 'information', 'here']\n\nprint(f\"Doc 1 Match: {boolean_match(query, doc1)}\")\nprint(f\"Doc 2 Match: {boolean_match(query, doc2)}\")",
                "input": "Query: ['information', 'retrieval']\nDoc1: [...information, retrieval...]\nDoc2: [...information...]",
                "output": "Doc 1 Match: True\nDoc 2 Match: False",
                "interpretation": "This demonstrates the strict Boolean model. \nDoc 1 matches because it contains ALL query terms.\nDoc 2 fails (False) because it is missing the term 'retrieval'.\nThis 'all-or-nothing' approach is the baseline for IR models."
            }
        },
        {
            "slideNumber": 14,
            "type": "research_perspective",
            "title": "Research Perspective: Advanced Modeling in Classic IR",
            "content": {
                "researchQuestions": [
                    {
                        "question": "The Orthogonality Assumption in Vector Space Models: How does term dependency undermine the independence assumption of the basis vectors?",
                        "answer": "The standard VSM assumes terms are independent (basis vectors are orthogonal). However, semantic dependencies (e.g., 'car' and 'engine' appearing together) invalidate this. Research in the 'Generalized Vector Space Model' (GVSM) solves this by redefining the inner product space using a Term-Term Correlation Matrix (G), allowing the system to match documents that are semantically related even if they share few exact terms."
                    },
                    {
                        "question": "The Curse of Dimensionality: Does Cosine Similarity remain a valid metric in ultra-high dimensional sparse vocabularies?",
                        "answer": "In high-dimensional spaces, the distance between any two random points tends to converge, making discrimination difficult. Research shows that for sparse IR vectors, the 'Fractional Distance Metric' or 'Manhattan Distance (L1)' can sometimes provide more robust ranking than Euclidean (L2) or Cosine, particularly when dealing with long-tail vocabulary distributions."
                    }
                ],
                "mathematicalModeling": {
                    "problemStatement": "Deriving TF-IDF as a measure of 'Self-Information' from Shannon's Information Theory.",
                    "derivation": [
                        {
                            "step": "Define the probability of a term t appearing in a random document d.",
                            "equation": "P(t) = \\frac{df_t}{N}",
                            "interpretation": "Common words have high probability; rare words have low probability."
                        },
                        {
                            "step": "Calculate the Self-Information (Surprisal) of term t.",
                            "equation": "I(t) = -\\log_2 P(t) = \\log_2 \\frac{N}{df_t}",
                            "interpretation": "This is the mathematical foundation of IDF. A term that appears rarely provides more 'Information' when it is found."
                        },
                        {
                            "step": "Combine with Term Frequency to get the total information contribution of term t to document d.",
                            "equation": "w_{t,d} = tf_{t,d} \\times (-\\log \\frac{df_t}{N})",
                            "interpretation": "TF-IDF is essentially the total 'Information Content' of a term within a specific document relative to the whole corpus."
                        }
                    ],
                    "conclusion": "Classic models like VSM and TF-IDF are not mere heuristics; they are grounded in Geometric Algebra and Information Theory, providing a rigourous framework for ranking unstructured text."
                }
            }
        },
        {
            "slideNumber": 15,
            "type": "summary",
            "title": "Topic Wrap-up",
            "content": {
                "linkage": "VSM is powerful but treats terms as independent dimensions. Next, we look at Fuzzy Logic and Neural alternatives.",
                "nextTopic": "Set-Theoretic Models (Fuzzy & Extended Boolean)",
                "preparation": "Review Fuzzy Set Theory (Membership functions)."
            }
        }
    ]
}