{
    "id": "u1-t1",
    "title": "Basic IR Concepts",
    "unitId": "unit-1",
    "slides": [
        {
            "slideNumber": 1,
            "type": "title",
            "title": "Basic Concepts of Information Retrieval",
            "subtitle": "Unit 1: Introduction to IR",
            "content": {
                "text": "Information Retrieval is the art and science of searching for information in documents, searching for documents themselves, searching for metadata which describe documents, or searching within databases, whether relational stand-alone databases or hypertextually-networked databases such as the World Wide Web.",
                "hook": "Imagine trying to find a needle in a haystack. Now imagine the haystack is the size of the internet. That's IR."
            }
        },
        {
            "slideNumber": 2,
            "type": "standard",
            "title": "Course Plan Positioning",
            "content": {
                "position": "Unit 1 / Topic 1",
                "preRequisites": [
                    "Data Structures",
                    "Basic Probability"
                ],
                "outcomes": [
                    "Define Information Retrieval",
                    "Differentiate IR from DB",
                    "Understand the IR System Architecture"
                ]
            }
        },
        {
            "slideNumber": 3,
            "type": "list",
            "title": "Content Covered Today",
            "items": [
                "Definition of IR",
                "Data vs Information vs Knowledge",
                "The User Task",
                "Logical View of Documents",
                "Past, Present, and Future of IR"
            ]
        },
        {
            "slideNumber": 4,
            "type": "standard",
            "title": "Background of the Topic",
            "content": {
                "text": "Before search engines (Google, Bing), we had library card catalogs. The need to organize and retrieve knowledge dates back to the Library of Alexandria. In the digital age, 1950s punched cards gave way to modern inverted indices."
            }
        },
        {
            "slideNumber": 5,
            "type": "standard",
            "title": "Motivation - Why Explore IR?",
            "content": {
                "text": "We live in the Zettabyte Era. Data is useless if you can't find what you need. IR is the bridge between 'Data Overload' and 'Actionable Insight'."
            }
        },
        {
            "slideNumber": 6,
            "type": "standard",
            "title": "Problem Statement",
            "content": {
                "problem": "Unstructured Data Explosion",
                "consequence": "Without IR models, finding relevant information in unstructured text (emails, web pages, PDFs) is computationally impossible at scale."
            }
        },
        {
            "slideNumber": 7,
            "type": "standard",
            "title": "Core Concept Overview",
            "content": {
                "concept": "Relevance",
                "definition": "The key difference between DB (Exact Match) and IR (Best Match). IR systems rank results based on how likely they satisfy the user's information need."
            }
        },
        {
            "slideNumber": 8,
            "type": "standard",
            "title": "Technical Foundations",
            "content": {
                "text": "IR relies on Boolean Logic, Vector Spaces, and Probability Theory. It assumes documents can be represented as 'Bag of Words' or feature vectors."
            },
            "formula": {
                "equation": "P = \\frac{|Rel \\cap Ret|}{|Ret|} \\quad , \\quad R = \\frac{|Rel \\cap Ret|}{|Rel|}",
                "description": "Precision and Recall are the fundamental metrics for evaluating the effectiveness of an IR system.",
                "terms": [
                    {
                        "symbol": "P",
                        "meaning": "Precision - accuracy of the retrieved results (proportion of retrieved docs that are relevant)"
                    },
                    {
                        "symbol": "R",
                        "meaning": "Recall - completeness of the retrieved results (proportion of relevant docs retrieved)"
                    },
                    {
                        "symbol": "Rel",
                        "meaning": "The set of all Relevant documents in the collection"
                    },
                    {
                        "symbol": "Ret",
                        "meaning": "The set of all Retrieved documents for a given query"
                    },
                    {
                        "symbol": "Rel \\cap Ret",
                        "meaning": "Intersection: The set of relevant documents that were successfully retrieved"
                    }
                ],
                "calculation": {
                    "exampleTitle": "Evaluation Case Study",
                    "description": "In a collection of 100 docs, where 20 are relevant.",
                    "steps": [
                        {
                            "label": "Gather Stats",
                            "formula": "|Rel|=20, |Ret|=30",
                            "note": "System retrieved 30 docs."
                        },
                        {
                            "label": "Count Overlap",
                            "formula": "|Rel \\cap Ret|=15",
                            "note": "15 out of the 30 retrieved were actually relevant."
                        },
                        {
                            "label": "Calculate Precision",
                            "formula": "P = \\frac{15}{30} = 0.5",
                            "note": "50% of what was retrieved is relevant."
                        },
                        {
                            "label": "Calculate Recall",
                            "formula": "R = \\frac{15}{20} = 0.75",
                            "note": "75% of all relevant docs were captured."
                        }
                    ],
                    "input": "Rel:20, Ret:30, Hits:15",
                    "output": "P: 0.5, R: 0.75"
                }
            }
        },
        {
            "slideNumber": 9,
            "type": "diagram",
            "title": "The Classic IR System Architecture",
            "content": {
                "description": "Functional overview of a classic information retrieval system architecture.",
                "steps": [
                    "User Information Need",
                    "Query Formulation",
                    "Matching & Ranking",
                    "Document Collection Index",
                    "Relevance Evaluation"
                ]
            },
            "subTopics": [
                {
                    "title": "Module 1: The Input System",
                    "content": "Focuses on how the system receives and processes the user's information need into a machine-readable format.",
                    "subSubTopics": [
                        {
                            "title": "Information Need vs. Query",
                            "content": "An information need is a gap in a user's knowledge, while a query is the actual string of words typed into the system.",
                            "keyPoints": [
                                "Information Need: Abstract, mental state.",
                                "Query: Surface representation of the need.",
                                "Vocabulary Mismatch: One of the biggest challenges in IR."
                            ]
                        },
                        {
                            "title": "Query Processing",
                            "content": "Transforming raw strings into vectors or logical expressions.",
                            "examples": [
                                "Expanding 'watch' to include 'wristwatch' and 'timepiece'.",
                                "Removing punctuation and normalizing case."
                            ]
                        }
                    ]
                },
                {
                    "title": "Module 2: The Core Retrieval Engine",
                    "content": "The heart of the system where matching and ranking occur based on mathematical models.",
                    "subSubTopics": [
                        {
                            "title": "The Indexer",
                            "content": "Builds an efficient structure (usually an Inverted Index) to speed up searches.",
                            "keyPoints": [
                                "Inverted Index: Maps words to the list of documents they appear in.",
                                "Scale: Must handle millions of documents efficiently."
                            ]
                        },
                        {
                            "title": "The Ranking Algorithm",
                            "content": "Applies a scoring function to determine which documents are most likely to be relevant.",
                            "examples": [
                                "Calculating Cosine Similarity between query and doc vectors.",
                                "Using PageRank to determine document authority."
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "slideNumber": 10,
            "type": "diagram",
            "title": "Flow from Document Collection to Indexed Terms",
            "content": {
                "description": "The systematic pipeline for transforming raw document collections into a searchable index structure.",
                "steps": [
                    "Raw Documents (HTML/PDF/Text)",
                    "Lexical Analysis & Tokenization",
                    "Filtering (Stopword Removal)",
                    "Morphological Analysis (Stemming)",
                    "Final Indexed Terms (Vocabulary)"
                ]
            }
        },
        {
            "slideNumber": 11,
            "type": "case_study",
            "title": "Real-Time Case Study",
            "content": {
                "title": "The Evolution of Google Search",
                "problem": "Keyword spamming in late 90s.",
                "solution": "PageRank algorithm (Link Analysis).",
                "outcome": "Trustworthy, authoritative results prioritizing relevance over frequency."
            }
        },
        {
            "slideNumber": 12,
            "type": "grid",
            "title": "Applications",
            "items": [
                "Web Search Engines",
                "Digital Libraries",
                "Enterprise Search",
                "Recommender Systems"
            ]
        },
        {
            "slideNumber": 13,
            "type": "standard",
            "title": "Challenges & Limitations",
            "content": {
                "text": "Ambiguity in Natural Language (Polysemy, Synonymy). Scalability with index size. Privacy in personalized search."
            }
        },
        {
            "slideNumber": 14,
            "type": "standard",
            "title": "Research & Open Problems",
            "content": {
                "text": "Neural IR (BERT/Transformers), Cross-lingual Retrieval, Multimedia/Video Search, Explainable Ranking."
            }
        },
        {
            "slideNumber": 15,
            "type": "standard",
            "title": "Tool / Technology Mapping",
            "content": {
                "tools": [
                    "Elasticsearch",
                    "Apache Solr",
                    "Lucene",
                    "Python NLTK/SpaCy"
                ]
            }
        },
        {
            "slideNumber": 16,
            "type": "activity",
            "title": "Experiential Learning Activity",
            "content": {
                "activity": "Boolean Search Challenge",
                "description": "Go to Google. Use 'site:', 'filetype:pdf', and quote operators to find a specific research paper on 'Vector Space Model' published before 2000."
            }
        },
        {
            "slideNumber": 17,
            "type": "project",
            "title": "Project-Based Learning Idea",
            "content": {
                "idea": "Build a Mini-Search Engine",
                "input": "A folder of 50 text files.",
                "process": "Build an Inverted Index (Python dictionary).",
                "output": "A script that takes a keyword and returns filenames containing it."
            }
        },
        {
            "slideNumber": 18,
            "type": "quiz",
            "title": "Open-Ended Quiz Questions",
            "questions": [
                "Why is exact matching insufficient for text retrieval?",
                "How is 'Data' different from 'Information' in the context of IR?",
                "Explain the concept of 'Bag of Words'.",
                "What is the role of stopwords in preprocessing?",
                "Give an example of a navigational query."
            ],
            "answers": [
                "Exact matching fails because of synonymy (words with same meaning) and polysemy (words with multiple meanings), causing missed relevant docs or irrelevant ones.",
                "Data is raw facts/symbols; Information is processed data with meaning and relevance to a user's need.",
                "A representation where text is treated as a collection of words, disregarding grammar and order but keeping multiplicity.",
                "Stopwords (like 'the', 'is') are removed to reduce index size and noise, as they carry little semantic value.",
                "Searching 'facebook login' or 'youtube' - where the intent is to go to a specific site."
            ]
        },
        {
            "slideNumber": 19,
            "type": "python_demo",
            "title": "Python Demo: IR Metrics Calculation",
            "content": {
                "code": "def calculate_metrics(relevant_total, retrieved_total, relevant_retrieved):\n    precision = relevant_retrieved / retrieved_total if retrieved_total > 0 else 0\n    recall = relevant_retrieved / relevant_total if relevant_total > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    return {\"Precision\": round(precision, 2), \"Recall\": round(recall, 2), \"F1\": round(f1, 2)}\n\n# Case Study: Collection of 100 documents\n# 20 are relevant, system retrieved 30, and 15 of them were relevant\nstats = calculate_metrics(relevant_total=20, retrieved_total=30, relevant_retrieved=15)\nprint(f\"Evaluation Results: {stats}\")",
                "input": "RelevantDocs = 20\nRetrievedDocs = 30\nRelevantHits = 15",
                "output": "Evaluation Results: {'Precision': 0.5, 'Recall': 0.75, 'F1': 0.6}",
                "interpretation": "This script automates the evaluation of an IR system. \nPrecision (0.5) tells us that 50% of the results shown to the user are relevant.\nRecall (0.75) tells us that the system successfully found 75% of all existing relevant documents.\nThe F1-Score (0.6) provides a balanced harmonic mean of both metrics."
            }
        },
        {
            "slideNumber": 20,
            "type": "exam",
            "title": "Model Examination Questions",
            "questions": [
                "Define Information Retrieval. Explain the functional overview of an IR system with a block diagram. (10 Marks)",
                "Differentiate between Data Retrieval and Information Retrieval. (5 Marks)",
                "Write short notes on Digital Libraries. (5 Marks)"
            ]
        },
        {
            "slideNumber": 21,
            "type": "summary",
            "title": "Plan for Next Class",
            "content": {
                "nextTopic": "The Retrieval Process & Modeling",
                "linkage": "Now that we know WHAT IR is, next we learn HOW to formalize it mathematically.",
                "preparation": "Read about Set Theory basics."
            }
        }
    ]
}