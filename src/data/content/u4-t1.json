{
    "id": "u4-t1",
    "title": "Multimedia IR (Modeling, Languages, Trends)",
    "unitId": "unit-4",
    "slides": [
        {
            "slideNumber": 1,
            "type": "title",
            "title": "Multimedia IR",
            "subtitle": "Beyond Text: Modeling Media",
            "content": {
                "text": "How do we search for information that isn't written? Multimedia IR focuses on the retrieval of images, audio, and video by modeling their inherent features.",
                "hook": "Search for a song by humming, or a car by taking a photo."
            }
        },
        {
            "slideNumber": 2,
            "type": "standard",
            "title": "Data Modeling in Multimedia IR",
            "content": {
                "text": "Multimedia objects are stored as 'Feature Vectors'. We extract mathematical descriptors (colors, textures, audio frequencies) and represent them in a high-dimensional space.",
                "hook": "The Semantic Gap: The difference between raw pixels (computer view) and concepts like 'A happy dog' (human view)."
            },
            "formula": {
                "equation": "d(\\vec{x}, \\vec{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}",
                "description": "Euclidean Distance is the standard metric used to measure similarity between multimedia feature vectors in a multi-dimensional space.",
                "terms": [
                    {
                        "symbol": "d(\\vec{x}, \\vec{y})",
                        "meaning": "Euclidean distance between example vector x and database vector y"
                    },
                    {
                        "symbol": "x_i",
                        "meaning": "The i-th feature value of the query object (e.g., average red channel brightness)"
                    },
                    {
                        "symbol": "y_i",
                        "meaning": "The i-th feature value of the stored document"
                    },
                    {
                        "symbol": "n",
                        "meaning": "Number of dimensions (number of extracted features)"
                    }
                ],
                "calculation": {
                    "exampleTitle": "Image Similarity Check",
                    "description": "Comparing two images based on 2 color features: [Red, Green].",
                    "steps": [
                        {
                            "label": "Extract Features",
                            "formula": "Img_{Q} = [0.8, 0.2], Img_{D} = [0.7, 0.3]",
                            "note": "Normalized color values (0-1 range)."
                        },
                        {
                            "label": "Calculate Differences",
                            "formula": "\\Delta R = 0.1, \\Delta G = -0.1",
                            "note": "Subtracting corresponding features."
                        },
                        {
                            "label": "Sum of Squares",
                            "formula": "(0.1)^2 + (-0.1)^2 = 0.01 + 0.01 = 0.02",
                            "note": "Squaring ensures positive values."
                        },
                        {
                            "label": "Final Distance",
                            "formula": "\\sqrt{0.02} \\approx 0.141",
                            "note": "Lower distance means higher similarity."
                        }
                    ],
                    "input": "Q:[0.8, 0.2], D:[0.7, 0.3]",
                    "output": "Distance: 0.141"
                }
            }
        },
        {
            "slideNumber": 3,
            "type": "standard",
            "title": "Multimedia Query Languages",
            "content": {
                "text": "Traditional keyword queries don't work for bitmaps. 1. Query by Example (QBE): Providing a sample image. 2. Query by Sketch: Drawing a shape. 3. Query by Humming: Measuring audio frequency shifts.",
                "hook": "Multimedia queries are visual and acoustic."
            }
        },
        {
            "slideNumber": 4,
            "type": "activity",
            "title": "Activity: QBE Mockup",
            "content": {
                "activity": "The Reverse Search",
                "description": "Imagine you want to find the name of a flower from a photo. Describe the 'Query Specification' process. How would the feature extraction look?"
            }
        },
        {
            "slideNumber": 5,
            "type": "quiz",
            "title": "Topic Quiz",
            "questions": [
                "What is the 'Semantic Gap'?",
                "Define 'Query by Example'.",
                "Why are feature vectors used for multimedia?",
                "Name one trend in multimedia IR."
            ]
        },
        {
            "slideNumber": 6,
            "type": "python_demo",
            "title": "Python Demo: Seed-based Crawler Frontier",
            "content": {
                "code": "frontier = [\"http://google.com\", \"http://wikipedia.org\"]\nvisited = set()\n\ndef crawl_step():\n    if not frontier: return\n    url = frontier.pop(0)\n    if url in visited: return\n    \n    print(f\"Fetching: {url}\")\n    visited.add(url)\n    # Simulating found links\n    new_links = [url + \"/about\", url + \"/contact\"]\n    frontier.extend(new_links)\n\ncrawl_step()\nprint(f\"Frontier Size: {len(frontier)}\")",
                "input": "Seeds: [google.com, wikipedia.org]",
                "output": "Fetching: http://google.com\nFrontier Size: 3",
                "interpretation": "Web Crawling starts with 'Seed' URLs (Uniform Resource Locators).\nThe 'Frontier' (Queue) tracks what to visit next.\nWe use a 'Visited' set to avoid infinite loops between pages."
            }
        },
        {
            "slideNumber": 7,
            "type": "research_perspective",
            "title": "Research Perspective: High-Dimensional Geometry & Neural Features",
            "content": {
                "researchQuestions": [
                    {
                        "question": "The Curse of Dimensionality: Why does Euclidean Distance become a poor similarity metric as we extract more features?",
                        "answer": "In high-dimensional space (e.g., 1000+ features from a ResNet model), the ratio between the distance to the nearest neighbor and the farthest neighbor converges to 1. This is the 'Concentration of Measure' phenomenon. Research focuses on 'Manifold Learning' (e.g., t-SNE, UMAP) to project these high-D vectors onto a lower-dimensional non-linear manifold where distance is more semantically meaningful."
                    },
                    {
                        "question": "Feature Fusion: How can we mathematically combine visual features and acoustic features into a single 'Multi-modal' ranking?",
                        "answer": "Research explores 'Early Fusion' (combining raw vectors) vs 'Late Fusion' (merging scores). A common research model is 'Canonical Correlation Analysis' (CCA), which identifies a shared latent subspace where visual and text vectors are maximally correlated, allowing us to 'Search for an image using a text description' through cross-modal alignment."
                    }
                ],
                "mathematicalModeling": {
                    "problemStatement": "Modeling Image Feature Extraction as a Convolutional Neural Network (CNN) Transformation.",
                    "derivation": [
                        {
                            "step": "Model the image $I$ as a 3D tensor and the feature extraction as a series of convolutional filters $K$.",
                            "equation": "F(I, K) = \\sum_{m,n} I(m,n) \\cdot K(i-m, j-n)",
                            "interpretation": "Convolution calculates the dot product between a sliding window of pixels and a learned weight matrix (kernel) that responds to specific patterns like edges or textures."
                        },
                        {
                            "step": "Apply a non-linear activation (ReLU) and Pooling to achieve spatial invariance.",
                            "equation": "h = \\max(0, F(I, K)) \\xrightarrow{Pooling} \\vec{v}_{feature}",
                            "interpretation": "Pooling reduces the spatial resolution while preserving the most 'active' features, ensuring that the system can recognize a dog regardless of its position in the photo."
                        },
                        {
                            "step": "Measure similarity in the Neural Embedding Space using Cosine Proximity.",
                            "equation": "S(I_1, I_2) = \\frac{\\Phi(I_1) \\cdot \\Phi(I_2)}{|\\Phi(I_1)| |\\Phi(I_2)|}",
                            "interpretation": "The 'Semantic Gap' is bridged by shifting from raw pixel distance to distance in the deep feature space $\\Phi$, which captures high-level conceptual similarity."
                        }
                    ],
                    "conclusion": "Multimedia IR is the ultimate frontier of retrieval, where the system must mathematically 'Interpret' the raw signal of the physical world to match it with human concept space."
                }
            }
        },
        {
            "slideNumber": 8,
            "type": "summary",
            "title": "Trends & Wrap-up",
            "content": {
                "text": "Trends include Deep Learning (CNNs), Generative AI search (Text-to-Image), and Multi-modal retrieval (Text+Image).",
                "nextTopic": "Multimedia Indexing & Searching",
                "preparation": "Recall image processing basics."
            }
        }
    ]
}