{
    "id": "u1-t9",
    "title": "Trends & Research Issues",
    "unitId": "unit-1",
    "slides": [
        {
            "slideNumber": 1,
            "type": "title",
            "title": "Trends & Research Issues in IR",
            "subtitle": "The Future of Search",
            "content": {
                "text": "Information Retrieval is not a solved problem. It changes with technology, user behavior, and data scale.",
                "hook": "Search is no longer just 10 blue links on a white page."
            }
        },
        {
            "slideNumber": 2,
            "type": "standard",
            "title": "Current Trending Topics",
            "content": {
                "text": "Neural Information Retrieval (BERT, GPT integration) is transforming search from exact keyword matching to semantic meaning matching.",
                "outcomes": [
                    "Dense retrieval (vectors)",
                    "Cross-lingual search",
                    "Conversational AI"
                ]
            },
            "formula": {
                "equation": "P(y_i|x) = \\frac{e^{s_i}}{\\sum_{j=1}^{K} e^{s_j}}",
                "description": "In Neural IR, the Softmax function is used to convert raw similarity scores (logits) from a deep learning model into a probability distribution over the top document candidates.",
                "terms": [
                    {
                        "symbol": "P(y_i|x)",
                        "meaning": "The probability that document 'i' is the relevant answer for query 'x'"
                    },
                    {
                        "symbol": "s_i",
                        "meaning": "Similarity score (logit) for document 'i' calculated by the neural network"
                    },
                    {
                        "symbol": "e",
                        "meaning": "Euler's number (exponential base)"
                    },
                    {
                        "symbol": "K",
                        "meaning": "Number of candidate documents in the final ranking stage"
                    }
                ],
                "calculation": {
                    "exampleTitle": "Neural Ranking Normalization",
                    "description": "Converting scores for 2 documents into probabilities.",
                    "steps": [
                        {
                            "label": "Raw Neural Scores",
                            "formula": "s_1 = 2.0, s_2 = 1.0",
                            "note": "Doc 1 is significantly stronger than Doc 2."
                        },
                        {
                            "label": "Exponentiate",
                            "formula": "e^2 \\approx 7.39, e^1 \\approx 2.72",
                            "note": "Applying the exponential function."
                        },
                        {
                            "label": "Calculate Sum",
                            "formula": "\\sum = 7.39 + 2.72 = 10.11",
                            "note": "The normalization factor."
                        },
                        {
                            "label": "Final Probability",
                            "formula": "P(1) = 0.73, P(2) = 0.27",
                            "note": "Probabilities sum to 1.0 (73% vs 27%)."
                        }
                    ],
                    "input": "Scores:[2, 1]",
                    "output": "P1: 0.73, P2: 0.27"
                }
            }
        },
        {
            "slideNumber": 3,
            "type": "standard",
            "title": "The Semantic Web",
            "content": {
                "definition": "Web 3.0. Moving from a 'Web of Documents' to a 'Web of Data'.",
                "text": "Using RDF, SPARQL, and Ontologies to help machines understand the *meaning* of data, not just keyword matches."
            }
        },
        {
            "slideNumber": 4,
            "type": "standard",
            "title": "Scalability Challenges",
            "content": {
                "problem": "The Web is infinite.",
                "text": "Indexing billions of dynamic pages in real-time requires distributed systems (MapReduce, Hadoop) and efficient compression."
            }
        },
        {
            "slideNumber": 5,
            "type": "diagram",
            "title": "Federated Search Architecture",
            "content": {
                "description": "Searching across multiple distinct databases with one query.",
                "steps": [
                    "User Query Entry",
                    "Database Source Selection",
                    "Protocol Translation (SRU/Z39.50)",
                    "Sub-Query Execution",
                    "Result Merging & De-duplication"
                ]
            }
        },
        {
            "slideNumber": 6,
            "type": "standard",
            "title": "Privacy vs Personalization",
            "content": {
                "text": "Better search requires knowing the user (Location, History). This conflicts with privacy concerns (GDPR). Research focuses on 'Privacy-Preserving IR'.",
                "hook": "Can we personalize without tracking?"
            }
        },
        {
            "slideNumber": 7,
            "type": "activity",
            "title": "Activity: Bias Hunt",
            "content": {
                "activity": "Algorithm Auditing",
                "description": "Search for professional roles (e.g., 'CEO', 'Doctor', 'Nurse') on Google Images. Analyze the gender and racial distribution of the top 20 results. Discuss potential algorithmic bias."
            }
        },
        {
            "slideNumber": 8,
            "type": "project",
            "title": "Research Proposal",
            "content": {
                "idea": "Define a Problem",
                "input": "Pick a domain (e.g., Medical Search).",
                "process": "Identification of gap (e.g., 'Doctors use jargon, Patients use slang').",
                "output": "Propose a feature to solve it (e.g., 'Automatic Query Expansion with Medical Thesaurus')."
            }
        },
        {
            "slideNumber": 9,
            "type": "quiz",
            "title": "Course Checkpoint",
            "questions": [
                "What is the difference between specific and federated search?",
                "Why is 'Context' becoming more important than 'Keywords'?",
                "Name one challenge in Voice Search compared to Text Search.",
                "What is the 'Vocabulary Mismatch Problem'?"
            ],
            "answers": [
                "Specific search indexes a centralized collection; Federated search broadcasts a query to multiple autonomous databases and merges the results.",
                "Because mobile/voice queries (\"restaurants near me\") rely on location, time, and user history (context) rather than just the explicit words typed.",
                "Lack of explicit punctuation/segmentation, speech recognition errors, and the expectation of a single direct answer rather than a list of links.",
                "The issue where the user uses different words (e.g., \"cellphone\") than the author used (e.g., \"mobile device\"), leading to missed relevant documents."
            ]
        },
        {
            "slideNumber": 10,
            "type": "python_demo",
            "title": "Python Demo: Histogram Intersection",
            "content": {
                "code": "def histogram_intersection(h1, h2):\n    return sum(min(a, b) for a, b in zip(h1, h2))\n\n# Simulating 4-bin color histograms\nimg1 = [0.1, 0.5, 0.2, 0.2] # Mostly Green\nimg2 = [0.2, 0.4, 0.1, 0.3] # Similar\n\nsim = histogram_intersection(img1, img2)\nprint(f\"Image Similarity: {round(sim, 2)}\")",
                "input": "Img1 Hist: [0.1, 0.5, 0.2, 0.2]\nImg2 Hist: [0.2, 0.4, 0.1, 0.3]",
                "output": "Image Similarity: 0.8",
                "interpretation": "For Multimedia IR, we compare feature vectors like color histograms.\nHistogram Intersection calculates the overlap between two distributions.\nA score of 0.8 indicates 80% color distribution similarity."
            }
        },
        {
            "slideNumber": 11,
            "type": "summary",
            "title": "Unit Wrap-up",
            "content": {
                "linkage": "We have completed the Introduction to IR (Unit 1). We know the 'What', 'Why', and 'How' of models.",
                "nextTopic": "Unit 2: Query Languages & Text Processing",
                "preparation": "Get ready to dive deep into SQL-like syntax and strict text operations."
            }
        }
    ]
}